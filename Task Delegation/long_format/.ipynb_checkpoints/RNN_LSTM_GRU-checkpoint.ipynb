{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660e6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "import traceback  # Added for error tracking\n",
    "import gc  # Added for garbage collection\n",
    "from tensorflow.keras.optimizers import Adam  # Added explicit Adam import\n",
    "import logging  # Added for logging\n",
    "from pathlib import Path  # Added for better path handling\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure TensorFlow logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "     \n",
    "def create_model(input_shape, units=32, n_layers=1, dropout=0.0, learning_rate=0.001,\n",
    "                 activation='relu', optimizer='adam', loss='mean_squared_error'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(units, activation=activation, input_shape=input_shape))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=True, \n",
    "                      input_shape=input_shape))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(units, activation=activation))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    elif n_layers == 3:\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=True, \n",
    "                      input_shape=input_shape))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(units, activation=activation))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Handle optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer.lower() == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=loss, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_lstm_model(df, dataset_name, note, randomsearch=True):\n",
    "    try:\n",
    "        print(f\"\\nStarting LSTM training for {dataset_name}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        data = df.copy()\n",
    "        \n",
    "        # Map visits to numbers for proper sorting\n",
    "        visit_map = {'BL': 0, 'V04': 1, 'V06': 2, 'V08': 3, 'V10': 4, 'V12': 5}\n",
    "        data['visit_num'] = data['visit'].map(visit_map)\n",
    "        if data['visit_num'].isnull().any():\n",
    "            print(\"Warning: Some visits couldn't be mapped\")\n",
    "            \n",
    "        # Sort by visit_num to ensure chronological order\n",
    "        data = data.sort_values(['visit_num'])\n",
    "        \n",
    "        # Drop visit_num as we won't use it as a feature\n",
    "        data = data.drop('visit_num', axis=1)\n",
    "        \n",
    "        # Get features\n",
    "        feature_cols = [col for col in data.columns if col not in ['visit', 'UPDRS3_total']]\n",
    "        \n",
    "        # Prepare sequences\n",
    "        X, y, n_patients_info = prepare_sequences(data, feature_cols)\n",
    "        print(f\"Number of sequences: {X.shape[0]}\")\n",
    "        print(f\"Sequence length: {X.shape[1]}\")\n",
    "        print(f\"Number of features: {X.shape[2]}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        print(f\"Training sequences: {X_train.shape[0]}\")\n",
    "        print(f\"Testing sequences: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "        X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "        \n",
    "        # Combine back with visit_num\n",
    "        X_train = np.dstack((X_train_features, X_train_visit))\n",
    "        X_test = np.dstack((X_test_features, X_test_visit))\n",
    "        \n",
    "        # Get input shape for model\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        \n",
    "        search_start_time = time.time()\n",
    "        if randomsearch:\n",
    "            best_params = perform_random_search(X_train, y_train, input_shape, model, n_iter=100)\n",
    "        else:\n",
    "            best_params = perform_grid_search(X_train, y_train, input_shape)\n",
    "        \n",
    "        search_end_time = time.time()\n",
    "        search_time = search_end_time - search_start_time\n",
    "        \n",
    "        # Train final model\n",
    "        # Create a copy of best_params and remove batch_size\n",
    "        model_params = best_params.copy()\n",
    "        batch_size = model_params.pop('batch_size')  # Remove and store batch_size\n",
    "\n",
    "        # Create model with only architecture parameters\n",
    "        final_model = create_model(input_shape, **model_params)\n",
    "\n",
    "        # Use batch_size in fit\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        history = final_model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[EarlyStopping(patience=10)],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        training_end_time = time.time()\n",
    "        training_time = training_end_time - training_start_time\n",
    "        \n",
    "        print('final model training done')\n",
    "        # Evaluate\n",
    "        y_pred = final_model.predict(X_test)\n",
    "        # Save all results\n",
    "        rmse, mae, r2 = save_model_results(\n",
    "            final_model, history, best_params, dataset_name, note,\n",
    "            X_train, X_test, y_test, y_pred, training_time\n",
    "        )\n",
    "        \n",
    "        return final_model, history, best_params, scaler, rmse, mae, r2\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        traceback.print_exc()  # Add this to get detailed error information\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "422b9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(X_train, y_train, input_shape):\n",
    "    print(\"performing grid search\")\n",
    "    # Early stopping callback\n",
    "    early_stopper = EarlyStopping(\n",
    "        monitor='loss',\n",
    "        min_delta=0.01,\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'units': [36, 72, 128],\n",
    "        'n_layers': [1, 2, 3],                 \n",
    "        'batch_size': [24, 64],\n",
    "        'epochs': [500],\n",
    "        'dropout': [0.2, 0.4],\n",
    "        'learning_rate': [0.001],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'optimizer': ['adam', 'rmsprop'],     \n",
    "        'loss': ['mean_squared_error', 'huber']\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=KerasRegressor(build_fn=create_model, input_shape=input_shape),\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    # Pass callbacks to fit\n",
    "    grid_search.fit(X_train, y_train, callbacks=[early_stopper])\n",
    "    \n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ae21779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_search(X_train, y_train, input_shape, n_iter=10):\n",
    "    print(\"performing random search\")\n",
    "    \n",
    "    param_distributions = {\n",
    "        'units': [36, 72, 128],\n",
    "        'n_layers': [1, 2, 3],\n",
    "        'batch_size': [24, 64],\n",
    "        'dropout': [0.2, 0.4],\n",
    "        'learning_rate': [0.001],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'optimizer': ['adam', 'rmsprop'],\n",
    "        'loss': ['mean_squared_error', 'huber']\n",
    "    }\n",
    "\n",
    "    model = KerasRegressor(\n",
    "        model=create_model,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    return random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5863dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_results(final_model, history, best_params, dataset_name, note, X_train, X_test, y_test, y_pred, training_time):\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Create results row\n",
    "    new_row = pd.DataFrame({\n",
    "        'dataset_name': [dataset_name],\n",
    "        'search_type': ['random' if randomsearch else 'grid'],\n",
    "        'model_architecture': [str(best_params)],\n",
    "        'test_rmse': [rmse],\n",
    "        'test_mae': [mae],\n",
    "        'test_r2': [r2],\n",
    "        \n",
    "        # Training details\n",
    "        'epochs_trained': [len(history.history['loss'])],\n",
    "        'best_epoch': [np.argmin(history.history['val_loss']) + 1],\n",
    "        'training_time': [training_time],\n",
    "        \n",
    "        # Best validation metrics\n",
    "        'best_val_loss': [min(history.history['val_loss'])],\n",
    "        'final_val_loss': [history.history['val_loss'][-1]],\n",
    "        \n",
    "        # Dataset characteristics\n",
    "        'train_size': [len(X_train)],\n",
    "        'test_size': [len(X_test)],\n",
    "        'n_features': [X_train.shape[2]],\n",
    "        'sequence_length': [X_train.shape[1]],\n",
    "        \n",
    "        # Training curves\n",
    "        'training_loss_curve': [str(history.history['loss'])],\n",
    "        'val_loss_curve': [str(history.history['val_loss'])]\n",
    "    })\n",
    "\n",
    "    # Load existing results or create new file\n",
    "    results_filename = f'lstm_results_{note}.xlsx'\n",
    "    try:\n",
    "        existing_results = pd.read_excel(results_filename)\n",
    "        updated_results = pd.concat([existing_results, new_row], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        updated_results = new_row\n",
    "\n",
    "    # Save updated results\n",
    "    updated_results.to_excel(results_filename, index=False)\n",
    "\n",
    "    # Save training curves plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training Curves for {dataset_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'training_curves_{dataset_name}_{note}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save actual vs predicted plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Actual vs Predicted for {dataset_name}')\n",
    "    plt.savefig(f'prediction_scatter_{dataset_name}_{note}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save model architecture diagram\n",
    "    try:\n",
    "        from tensorflow.keras.utils import plot_model\n",
    "        plot_model(final_model, to_file=f'model_architecture_{dataset_name}_{note}.png', \n",
    "                  show_shapes=True, show_layer_names=True)\n",
    "    except ImportError:\n",
    "        print(\"Couldn't save model architecture diagram - graphviz might be missing\")\n",
    "\n",
    "    # Save the model\n",
    "    final_model.save(f'model_{dataset_name}_{note}.h5')\n",
    "\n",
    "    print(f\"\\nResults saved for {dataset_name}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test MAE: {mae:.4f}\")\n",
    "    print(f\"Test R2: {r2:.4f}\")\n",
    "\n",
    "    return rmse, mae, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a2cadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, feature_cols):\n",
    "    \"\"\"\n",
    "    Prepare input sequences and targets for LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input dataframe with multiindex\n",
    "        feature_cols (list): List of feature column names\n",
    "        \n",
    "    Returns:\n",
    "        X (np.array): Input sequences\n",
    "        y (np.array): Target values\n",
    "        n_patients_info (dict): Dictionary containing patient statistics\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    patients = data.index.unique()\n",
    "    n_patients = len(patients)\n",
    "    n_timesteps = len(data['visit'].unique()) - 1\n",
    "    n_features = len(feature_cols)\n",
    "    \n",
    "    print(f\"\\nInitial dimensions:\")\n",
    "    print(f\"Total patients: {n_patients}\")\n",
    "    print(f\"Timesteps: {n_timesteps}\")\n",
    "    print(f\"Features: {n_features}\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    X = np.zeros((n_patients, n_timesteps, n_features))\n",
    "    y = np.zeros(n_patients)\n",
    "    \n",
    "    # Determine last visit for each patient\n",
    "    last_visit = data.groupby(level=0)['visit_num'].max()\n",
    "    print(f\"\\nLast visit distribution:\\n{last_visit.value_counts().sort_index()}\")\n",
    "    \n",
    "    # Fill arrays\n",
    "    valid_patients = 0\n",
    "    skipped_patients = 0\n",
    "    \n",
    "    for i, patient in enumerate(patients):\n",
    "        patient_data = data.loc[patient].sort_values('visit_num')\n",
    "        if isinstance(patient_data, pd.Series):\n",
    "            patient_data = pd.DataFrame([patient_data])\n",
    "        \n",
    "        patient_last_visit = last_visit[patient]\n",
    "        \n",
    "        # Separate input and target\n",
    "        input_data = patient_data[patient_data['visit_num'] < patient_last_visit]\n",
    "        target_data = patient_data[patient_data['visit_num'] == patient_last_visit]['UPDRS3_total'].values[0]\n",
    "        \n",
    "        # Check if we have enough data points\n",
    "        if len(input_data) < n_timesteps:\n",
    "            skipped_patients += 1\n",
    "            continue\n",
    "        \n",
    "        # Store the sequences\n",
    "        X[valid_patients] = input_data[feature_cols].values\n",
    "        y[valid_patients] = target_data\n",
    "        \n",
    "        # Debug prints for first few patients\n",
    "        if valid_patients < 3:\n",
    "            print(f\"\\nPatient {patient}:\")\n",
    "            print(f\"Input sequence shape: {input_data.shape}\")\n",
    "            print(f\"Input visits: {input_data['visit'].values}\")\n",
    "            print(f\"Target visit: {patient_data[patient_data['visit_num'] == patient_last_visit]['visit'].values[0]}\")\n",
    "            print(f\"Input sequence visit_nums: {input_data['visit_num'].values}\")\n",
    "            print(f\"Target value (UPDRS3_total): {target_data}\")\n",
    "        \n",
    "        valid_patients += 1\n",
    "    \n",
    "    # Trim arrays to remove unused rows\n",
    "    X = X[:valid_patients]\n",
    "    y = y[:valid_patients]\n",
    "    \n",
    "    # Prepare patient statistics\n",
    "    n_patients_info = {\n",
    "        'total': len(patients),\n",
    "        'skipped': skipped_patients,\n",
    "        'valid': valid_patients\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPatient statistics:\")\n",
    "    print(f\"Total patients: {n_patients_info['total']}\")\n",
    "    print(f\"Skipped patients: {n_patients_info['skipped']}\")\n",
    "    print(f\"Valid patients: {n_patients_info['valid']}\")\n",
    "    print(f\"Final X shape: {X.shape}\")\n",
    "    print(f\"Final y shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, n_patients_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6afd6933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dummy Dataset:\n",
      "      visit  visit_num    feature1   feature2   feature3  UPDRS3_total\n",
      "PATNO                                                                 \n",
      "1001     BL          0   91.015853  56.189082  75.962365            46\n",
      "1002     BL          0  118.314588  50.026218  66.002863            20\n",
      "1003     BL          0   82.868655  53.114250  77.658512            24\n",
      "1001    V04          1  104.919192  42.027862  79.115511            37\n",
      "1002    V04          1  111.794401  50.234903  62.727087            41\n",
      "1003    V04          1  113.538724  44.661898  69.012108            49\n",
      "1001    V06          2   86.797668  47.003125  80.692919            31\n",
      "1002    V06          2   95.308243  47.749673  85.221415            31\n",
      "1003    V06          2   98.854602  49.288103  87.409216            49\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9 entries, 1001 to 1003\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   visit         9 non-null      object \n",
      " 1   visit_num     9 non-null      int64  \n",
      " 2   feature1      9 non-null      float64\n",
      " 3   feature2      9 non-null      float64\n",
      " 4   feature3      9 non-null      float64\n",
      " 5   UPDRS3_total  9 non-null      int32  \n",
      "dtypes: float64(3), int32(1), int64(1), object(1)\n",
      "memory usage: 468.0+ bytes\n",
      "None\n",
      "\n",
      "Visit distribution per patient:\n",
      "PATNO  visit\n",
      "1001   BL       1\n",
      "       V04      1\n",
      "       V06      1\n",
      "1002   BL       1\n",
      "       V04      1\n",
      "       V06      1\n",
      "1003   BL       1\n",
      "       V04      1\n",
      "       V06      1\n",
      "Name: visit, dtype: int64\n",
      "\n",
      "Initial dimensions:\n",
      "Total patients: 3\n",
      "Timesteps: 2\n",
      "Features: 3\n",
      "\n",
      "Last visit distribution:\n",
      "2    3\n",
      "Name: visit_num, dtype: int64\n",
      "\n",
      "Patient 1001:\n",
      "Input sequence shape: (2, 6)\n",
      "Input visits: ['BL' 'V04']\n",
      "Target visit: V06\n",
      "Input sequence visit_nums: [0 1]\n",
      "Target value (UPDRS3_total): 31\n",
      "\n",
      "Patient 1002:\n",
      "Input sequence shape: (2, 6)\n",
      "Input visits: ['BL' 'V04']\n",
      "Target visit: V06\n",
      "Input sequence visit_nums: [0 1]\n",
      "Target value (UPDRS3_total): 31\n",
      "\n",
      "Patient 1003:\n",
      "Input sequence shape: (2, 6)\n",
      "Input visits: ['BL' 'V04']\n",
      "Target visit: V06\n",
      "Input sequence visit_nums: [0 1]\n",
      "Target value (UPDRS3_total): 49\n",
      "\n",
      "Patient statistics:\n",
      "Total patients: 3\n",
      "Skipped patients: 0\n",
      "Valid patients: 3\n",
      "Final X shape: (3, 2, 3)\n",
      "Final y shape: (3,)\n",
      "\n",
      "Output shapes:\n",
      "X shape: (3, 2, 3)\n",
      "y shape: (3,)\n",
      "\n",
      "First sequence (Patient 1001):\n",
      "Input features:\n",
      "[[ 91.01585329  56.18908156  75.96236505]\n",
      " [104.91919172  42.02786171  79.11551067]]\n",
      "\n",
      "Target value:\n",
      "31.0\n",
      "All validation checks passed!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Create dummy data\n",
    "# data = {\n",
    "#     'PATNO': [1001, 1001, 1001, \n",
    "#               1002, 1002, 1002,\n",
    "#               1003, 1003, 1003],\n",
    "#     'visit': ['BL', 'V04', 'V06'] * 3,\n",
    "#     'visit_num': [0, 1, 2] * 3,  # Added visit_num column\n",
    "#     'feature1': np.random.normal(100, 10, 9),  # Random values around 100\n",
    "#     'feature2': np.random.normal(50, 5, 9),    # Random values around 50\n",
    "#     'feature3': np.random.normal(75, 8, 9),    # Random values around 75\n",
    "#     'UPDRS3_total': np.random.randint(20, 50, 9)  # Random integers between 20-50\n",
    "# }\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Set index\n",
    "# df.set_index('PATNO', inplace=True)\n",
    "\n",
    "# # Sort by PATNO and visit\n",
    "# df = df.sort_index().sort_values(['visit_num'])\n",
    "\n",
    "# # Display the data\n",
    "# print(\"\\nDummy Dataset:\")\n",
    "# print(df)\n",
    "\n",
    "# # Verify the structure\n",
    "# print(\"\\nDataset Info:\")\n",
    "# print(df.info())\n",
    "\n",
    "# print(\"\\nVisit distribution per patient:\")\n",
    "# print(df.groupby(level=0)['visit'].value_counts().sort_index())\n",
    "\n",
    "# # Test prepare_sequences\n",
    "# feature_cols = ['feature1', 'feature2', 'feature3']\n",
    "# X, y, n_patients_info = prepare_sequences(df, feature_cols)\n",
    "\n",
    "# print(\"\\nOutput shapes:\")\n",
    "# print(f\"X shape: {X.shape}\")  # Should be (3, 2, 3) - 3 patients, 2 timesteps, 3 features\n",
    "# print(f\"y shape: {y.shape}\")  # Should be (3,) - target for 3 patients\n",
    "\n",
    "# # Display first sequence for verification\n",
    "# print(\"\\nFirst sequence (Patient 1001):\")\n",
    "# print(\"Input features:\")\n",
    "# print(X[0])\n",
    "# print(\"\\nTarget value:\")\n",
    "# print(y[0])\n",
    "\n",
    "# validate_sequence_data(df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a525bf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All validation checks passed!\n"
     ]
    }
   ],
   "source": [
    "def validate_sequence_data(df, X, y):\n",
    "    \"\"\"\n",
    "    Validates the prepared sequences against the original data\n",
    "    \"\"\"\n",
    "    # Check basic shapes\n",
    "    assert X.shape[0] == y.shape[0], \"Number of samples doesn't match\"\n",
    "    assert X.shape[0] == df.index.nunique(), \"Number of patients doesn't match\"\n",
    "    assert X.shape[2] == 3, \"Number of features doesn't match\"\n",
    "    \n",
    "    # Check sequence order\n",
    "    first_patient = df.index[0]\n",
    "    patient_data = df.loc[first_patient]\n",
    "    \n",
    "    # Verify first patient's sequence\n",
    "    assert np.array_equal(\n",
    "        patient_data.loc[patient_data['visit'].isin(['BL', 'V04'])]['visit_num'].values,\n",
    "        [0, 1]\n",
    "    ), \"Visit sequence is incorrect\"\n",
    "    \n",
    "    # Verify target value\n",
    "    assert y[0] == patient_data.loc[patient_data['visit'] == 'V06', 'UPDRS3_total'].values[0], \\\n",
    "        \"Target value doesn't match\"\n",
    "    \n",
    "    print(\"All validation checks passed!\")\n",
    "\n",
    "# Run validation\n",
    "validate_sequence_data(df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe58bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, SimpleRNN \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def train_rnn(df, dataset_name):\n",
    "    try:\n",
    "        print(f\"\\nStarting RNN training for {dataset_name}\")\n",
    "        \n",
    "        # Data prep\n",
    "        data = df.copy()\n",
    "        visit_map = {'BL': 0, 'V04': 1, 'V06': 2, 'V08': 3, 'V10': 4, 'V12': 5}\n",
    "        data['visit_num'] = data['visit'].map(visit_map)\n",
    "        data = data.sort_values(['visit_num'])\n",
    "        \n",
    "        # Get features\n",
    "        feature_cols = [col for col in data.columns if col not in ['visit', 'UPDRS3_total']]\n",
    "        \n",
    "        # Prepare sequences\n",
    "        X, y, n_patients_info = prepare_sequences(data, feature_cols)\n",
    "        \n",
    "        # Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scaling\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "        X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "        X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "        X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "        X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "\n",
    "        # Parameter distributions for random search\n",
    "        param_distributions = {\n",
    "            'units': [32, 64, 128, 256],\n",
    "            'batch_size': [16, 32, 64, 128],\n",
    "            'learning_rate': [1e-4, 5e-4, 1e-3, 5e-3, 1e-2],  # Fixed values instead of loguniform\n",
    "            'dropout': np.linspace(0, 0.5, 6),  # Fixed values instead of uniform\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'recurrent_dropout': np.linspace(0, 0.5, 6)  # Fixed values instead of uniform\n",
    "        }\n",
    "\n",
    "        n_iter = 30\n",
    "        results = []\n",
    "\n",
    "        # Random search\n",
    "        for i in range(n_iter):\n",
    "            params = {\n",
    "            'units': np.random.choice(param_distributions['units']),\n",
    "            'batch_size': np.random.choice(param_distributions['batch_size']),\n",
    "            'learning_rate': np.random.choice(param_distributions['learning_rate']),\n",
    "            'dropout': np.random.choice(param_distributions['dropout']),\n",
    "            'activation': np.random.choice(param_distributions['activation']),\n",
    "            'recurrent_dropout': np.random.choice(param_distributions['recurrent_dropout'])\n",
    "        }\n",
    "            \n",
    "            print(f\"\\nTrial {i+1}/{n_iter}\")\n",
    "            print(\"Parameters:\", params)\n",
    "            \n",
    "            model = Sequential([\n",
    "                SimpleRNN(\n",
    "                    units=params['units'],\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    activation=params['activation'],\n",
    "                    recurrent_dropout=params['recurrent_dropout']\n",
    "                ),\n",
    "                Dropout(params['dropout']),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "                loss='mse',\n",
    "                metrics=['mae']\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=params['batch_size'],\n",
    "                validation_split=0.2,\n",
    "                callbacks=[\n",
    "                    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "                ],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            train_metrics = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "            test_metrics = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "            \n",
    "            result = {\n",
    "                **params,\n",
    "                'train_mse': train_metrics[0],\n",
    "                'train_mae': train_metrics[1],\n",
    "                'test_mse': test_metrics[0],\n",
    "                'test_mae': test_metrics[1],\n",
    "                'epochs': len(history.history['loss']),\n",
    "                'val_loss': min(history.history['val_loss'])\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"Test MAE: {test_metrics[1]:.4f}\")\n",
    "\n",
    "        # Convert results to DataFrame and find best parameters\n",
    "        results_df = pd.DataFrame(results)\n",
    "        best_idx = results_df['test_mae'].idxmin()\n",
    "        best_params = results_df.iloc[best_idx].to_dict()\n",
    "\n",
    "        # Train final model with best parameters\n",
    "        final_model = Sequential([\n",
    "            SimpleRNN(\n",
    "                units=int(best_params['units']),\n",
    "                input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                activation=best_params['activation'],\n",
    "                recurrent_dropout=best_params['recurrent_dropout']\n",
    "            ),\n",
    "            Dropout(best_params['dropout']),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        final_model.compile(\n",
    "            optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        final_history = final_model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=int(best_params['batch_size']),\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print('final model training done')\n",
    "        # Evaluate\n",
    "        y_pred = final_model.predict(X_test_scaled)\n",
    "        rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results = pd.DataFrame({\n",
    "            'dataset_name': [dataset_name],\n",
    "            'best_params': [str(best_params)],\n",
    "            'test_rmse': [rmse],\n",
    "            'test_mae': [mae],\n",
    "            'test_r2': [r2]\n",
    "        })\n",
    "        \n",
    "        results.to_excel(f'rnn_results_{dataset_name}_{timestamp}.xlsx', index=False)\n",
    "        \n",
    "        print(f\"\\nResults saved for {dataset_name}\")\n",
    "        print(f\"Test RMSE: {rmse:.4f}\")\n",
    "        print(f\"Test MAE: {mae:.4f}\")\n",
    "        print(f\"Test R2: {r2:.4f}\")\n",
    "        \n",
    "        return final_model, final_history, best_params, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932e107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d000c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f0170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d6e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
